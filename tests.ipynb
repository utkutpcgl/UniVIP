{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms as T\n",
    "from torch import nn\n",
    "image_size=224\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "import torch\n",
    "AUG1 = T.Compose([\n",
    "        T.RandomResizedCrop((image_size, image_size)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        RandomApply(\n",
    "            T.ColorJitter(0.4, 0.4, 0.2, 0.1),\n",
    "            p = 0.8\n",
    "        ),\n",
    "        T.RandomGrayscale(p=0.2),\n",
    "        RandomApply(\n",
    "            T.GaussianBlur((23, 23)),\n",
    "            p = 1\n",
    "        ),\n",
    "        T.Normalize(\n",
    "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "            std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "    ]\n",
    ")\n",
    "\n",
    "AUG2 = T.Compose([\n",
    "        T.RandomResizedCrop((image_size, image_size)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        RandomApply(\n",
    "            T.ColorJitter(0.4, 0.4, 0.2, 0.1),\n",
    "            p = 0.8\n",
    "        ),\n",
    "        T.RandomGrayscale(p=0.2),\n",
    "        RandomApply(\n",
    "            T.GaussianBlur((23, 23)),\n",
    "            p = 0.1\n",
    "        ),\n",
    "        # T.RandomSolarize(0.5, p=0.2), # TODO threshold has to be defined according 0-1 or 0-255 input\n",
    "        T.Normalize(\n",
    "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "            std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS\n",
    "* BELOW I HAVE TESTED IF TRANSFORMS CALCULATE THE RANDOM PROBABILITY SEPERATELY FOR EVERY IMAGE IN A\n",
    "* ANSWER -> IT DOES NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T.RandomResizedCrop((image_size, image_size))\n",
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "params= t.get_params(img, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)) # returns top: int, left: int, height: int, width\n",
    "print(params)\n",
    "# TODO apply transformations with this transparency to get the intersection coordinates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "t_color = T.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "params = t_color.get_params((0.2, 1.8), (0.2, 1.8), (0.2, 1.8), (-0.2, 0.2))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "class RandomHorizontalFlip(torch.nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        prob = torch.rand(1)\n",
    "        print(prob)\n",
    "        if prob < self.p:\n",
    "            return F.hflip(img)\n",
    "        return img\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "a = RandomHorizontalFlip()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "print(img.shape)\n",
    "_, height, width = F.get_dimensions(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "def random_hflip(img, p):\n",
    "    flip_bool = torch.rand(1) < p\n",
    "    return (F.hflip(img),flip_bool) if flip_bool else (img, flip_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "def transform_image(image, image_size):\n",
    "    # Apply color jitter with probability 0.3\n",
    "    if random.random() < 0.3:\n",
    "        image = TF.adjust_brightness(image, brightness_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_contrast(image, contrast_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_saturation(image, saturation_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_hue(image, hue_factor=random.uniform(-0.2, 0.2))\n",
    "\n",
    "    # Apply grayscale with probability 0.2\n",
    "    if random.random() < 0.2:\n",
    "        image = TF.to_grayscale(image)\n",
    "\n",
    "    # Apply horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        flipped_bool = True\n",
    "        image = TF.hflip(image)\n",
    "\n",
    "    # Apply gaussian blur with probability 0.2\n",
    "    if random.random() < 0.2:\n",
    "        image = TF.gaussian_blur(image, kernel_size=3, sigma=(1.0, 2.0))\n",
    "\n",
    "    # Apply random resized crop\n",
    "    crop_coordinates = top, left, height, width = T.RandomResizedCrop.get_params(\n",
    "        image, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.))\n",
    "    # if size is int (not list) smaller edge will be scaled to match this.\n",
    "    image = TF.resized_crop(image, top, left, height, width, size=(image_size, image_size))\n",
    "\n",
    "    # Normalize the image\n",
    "    image = TF.normalize(\n",
    "        torch.tensor(image, dtype=torch.float32),\n",
    "        mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "        std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "\n",
    "    return image, flipped_bool, crop_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()         \n",
    "\n",
    "    def forward(self, x):\n",
    "        r = random.random()\n",
    "        print(\"Hi ma\")        \n",
    "        print(r)\n",
    "        if r > 0:\n",
    "            x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "n = Net()\n",
    "# r = n(torch.tensor(-1))\n",
    "# print(r)\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "r = n.forward(img) #not planned to call directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "conv = torch.nn.Conv2d(3, 4, 2)\n",
    "\n",
    "at = torch.ones((3,4,3,60,60))\n",
    "print(at.size())\n",
    "bt = at.reshape(-1,*at.size()[-3:])\n",
    "print(bt.size())\n",
    "conv(bt).shape\n",
    "instance = F.interpolate(bt, size=(96, 96), mode=\"bicubic\") # resize instance to 96x96\n",
    "instance.shape\n",
    "torch.stack([bt,bt]).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming O and T are your input matrices of size KxM and KxN respectively.\n",
    "# Horizontal stack of column vectors of lenght K (K is number of instances)\n",
    "O_matrix = torch.tensor([[3,5,6],[4,12,8]], dtype=torch.float)# (M is number of features) \n",
    "T_matrix = torch.tensor([[1,2,3],[4,2,5]], dtype=torch.float)# (N is number of features)\n",
    "\n",
    "# Step 1: Compute DotProductMatrix\n",
    "DotProductMatrix = torch.mm(O_matrix.t(), T_matrix)\n",
    "\n",
    "# Step 2: Compute Norm matrices\n",
    "NormMatrixO = torch.norm(O_matrix, dim=0, keepdim=True)\n",
    "NormMatrixT = torch.norm(T_matrix, dim=0, keepdim=True)\n",
    "\n",
    "# Step 3: Compute C\n",
    "C = 1 - (DotProductMatrix / (NormMatrixO.t() * NormMatrixT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming O and T are your input matrices of size KxM and KxN respectively.\n",
    "# Horizontal stack of column vectors of lenght K (K is number of instances)\n",
    "O = torch.tensor([[3,5,6],[4,12,8]], dtype=torch.float)# (M is number of features) \n",
    "T = torch.tensor([[1,2,3],[4,2,5]], dtype=torch.float)# (N is number of features)\n",
    "\n",
    "# Step 1: Compute DotProductMatrix\n",
    "DotProductMatrix = torch.mm(O.t(), T)\n",
    "O.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute Norm matrices, output will be a horizontal vector.\n",
    "\n",
    "NormMatrixO = torch.norm(O, dim=0, keepdim=True) # find norm of columns\n",
    "NormMatrixT = torch.norm(T, dim=0, keepdim=True)\n",
    "NormMatrixO.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormMatrixT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO check if this (NormMatrixO.t() * NormMatrixT) matches the target indices.\n",
    "# Step 3: Compute C\n",
    "C = 1 - (DotProductMatrix / (NormMatrixO.t() * NormMatrixT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a and b for sinkhorn iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define two matrices\n",
    "A = torch.ones(3,4,6,6)\n",
    "B = torch.ones(3,4,6,6)\n",
    "\n",
    "# Perform elementwise multiplication\n",
    "C = torch.mul(A, B)\n",
    "print(C.shape)\n",
    "\n",
    "# Sum the results\n",
    "result = torch.sum(C, dim=(-2,-1))\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4, 5],\n",
       "        [2, 3, 4, 5]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.tensor([2,3,4,5]).unsqueeze(0), torch.tensor([2,3,4,5]).unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 4), dtype=torch.int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1]).dtype\n",
    "torch.ones(0,4, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.ones(0,4), torch.tensor([2,3,4,5]).unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(torch.tensor([2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinkhornDistance(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "        Args:\n",
    "        eps (float): regularization coefficient\n",
    "        max_iter (int): maximum number of Sinkhorn iterations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-1, max_iter=50): # Default values from the paper OTA (and UniVIP)\n",
    "        super(SinkhornDistance, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def forward(self, mu, nu, C):\n",
    "        u = torch.ones_like(mu) # demander a\n",
    "        v = torch.ones_like(nu) # supplier b\n",
    "\n",
    "        # Sinkhorn iterations\n",
    "        for _ in range(self.max_iter):\n",
    "            # NOTE original algorithm first updates u then v. Hence modified.\n",
    "            u = self.eps * (torch.log(mu + 1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n",
    "            # transpose((-2, -1)) to avoid batch dimension\n",
    "            v = self.eps * (torch.log(nu + 1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n",
    "\n",
    "        U, V = u, v\n",
    "        # Transport plan pi = diag(a)*K*diag(b)\n",
    "        pi = torch.exp(self.M(C, U, V)).detach()\n",
    "        # Sinkhorn distance\n",
    "        # transpose((-2, -1)) to avoid batch dimension\n",
    "        cost = torch.sum(pi * C, dim=(-2, -1))\n",
    "        return cost, pi\n",
    "\n",
    "    def M(self, C, u, v):\n",
    "        '''\n",
    "        \"Modified cost for logarithmic updates\"\n",
    "        \"$M_{ij} = (-c_{ij} + u_i + v_j) / epsilon$\"\n",
    "        '''\n",
    "        # Original code updates (-C + u.unsqueeze(1) + v.unsqueeze(0)) / epsilon, but here we avoid batch dimension.\n",
    "        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n",
    "\n",
    "sinkhorn_distance = SinkhornDistance()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
