{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms as T\n",
    "from torch import nn\n",
    "image_size=224\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "import torch\n",
    "DEFAULT_AUG = T.Compose([\n",
    "        RandomApply(\n",
    "            T.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "            p = 0.3\n",
    "        ),\n",
    "        T.RandomGrayscale(p=0.2),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        RandomApply(\n",
    "            T.GaussianBlur((3, 3), (1.0, 2.0)),\n",
    "            p = 0.2\n",
    "        ),\n",
    "        T.RandomResizedCrop((image_size, image_size)),\n",
    "        T.Normalize(\n",
    "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "            std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS\n",
    "* BELOW I HAVE TESTED IF TRANSFORMS CALCULATE THE RANDOM PROBABILITY SEPERATELY FOR EVERY IMAGE IN A\n",
    "* ANSWER -> IT DOES NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T.RandomResizedCrop((image_size, image_size))\n",
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "params= t.get_params(img, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)) # returns top: int, left: int, height: int, width\n",
    "print(params)\n",
    "# TODO apply transformations with this transparency to get the intersection coordinates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "t_color = T.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "params = t_color.get_params((0.2, 1.8), (0.2, 1.8), (0.2, 1.8), (-0.2, 0.2))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "class RandomHorizontalFlip(torch.nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        prob = torch.rand(1)\n",
    "        print(prob)\n",
    "        if prob < self.p:\n",
    "            return F.hflip(img)\n",
    "        return img\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "a = RandomHorizontalFlip()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "print(img.shape)\n",
    "_, height, width = F.get_dimensions(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "def random_hflip(img, p):\n",
    "    flip_bool = torch.rand(1) < p\n",
    "    return (F.hflip(img),flip_bool) if flip_bool else (img, flip_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "def transform_image(image, image_size):\n",
    "    # Apply color jitter with probability 0.3\n",
    "    if random.random() < 0.3:\n",
    "        image = TF.adjust_brightness(image, brightness_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_contrast(image, contrast_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_saturation(image, saturation_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_hue(image, hue_factor=random.uniform(-0.2, 0.2))\n",
    "\n",
    "    # Apply grayscale with probability 0.2\n",
    "    if random.random() < 0.2:\n",
    "        image = TF.to_grayscale(image)\n",
    "\n",
    "    # Apply horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        flipped_bool = True\n",
    "        image = TF.hflip(image)\n",
    "\n",
    "    # Apply gaussian blur with probability 0.2\n",
    "    if random.random() < 0.2:\n",
    "        image = TF.gaussian_blur(image, kernel_size=3, sigma=(1.0, 2.0))\n",
    "\n",
    "    # Apply random resized crop\n",
    "    crop_coordinates = top, left, height, width = T.RandomResizedCrop.get_params(\n",
    "        image, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.))\n",
    "    # if size is int (not list) smaller edge will be scaled to match this.\n",
    "    image = TF.resized_crop(image, top, left, height, width, size=(image_size, image_size))\n",
    "\n",
    "    # Normalize the image\n",
    "    image = TF.normalize(\n",
    "        torch.tensor(image, dtype=torch.float32),\n",
    "        mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "        std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "\n",
    "    return image, flipped_bool, crop_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()         \n",
    "\n",
    "    def forward(self, x):\n",
    "        r = random.random()\n",
    "        print(\"Hi ma\")        \n",
    "        print(r)\n",
    "        if r > 0:\n",
    "            x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "n = Net()\n",
    "# r = n(torch.tensor(-1))\n",
    "# print(r)\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "r = n.forward(img) #not planned to call directly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
