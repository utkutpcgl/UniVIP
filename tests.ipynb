{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = {\"b\":1,\"a\":2}\n",
    "ad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms as T\n",
    "from torch import nn\n",
    "image_size=224\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "import torch\n",
    "AUG1 = T.Compose([\n",
    "        T.RandomResizedCrop((image_size, image_size)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        RandomApply(\n",
    "            T.ColorJitter(0.4, 0.4, 0.2, 0.1),\n",
    "            p = 0.8\n",
    "        ),\n",
    "        T.RandomGrayscale(p=0.2),\n",
    "        RandomApply(\n",
    "            T.GaussianBlur((23, 23)),\n",
    "            p = 1\n",
    "        ),\n",
    "        T.Normalize(\n",
    "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "            std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "    ]\n",
    ")\n",
    "\n",
    "AUG2 = T.Compose([\n",
    "        T.RandomResizedCrop((image_size, image_size)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        RandomApply(\n",
    "            T.ColorJitter(0.4, 0.4, 0.2, 0.1),\n",
    "            p = 0.8\n",
    "        ),\n",
    "        T.RandomGrayscale(p=0.2),\n",
    "        RandomApply(\n",
    "            T.GaussianBlur((23, 23)),\n",
    "            p = 0.1\n",
    "        ),\n",
    "        # T.RandomSolarize(0.5, p=0.2), # TODO threshold has to be defined according 0-1 or 0-255 input\n",
    "        T.Normalize(\n",
    "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "            std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS\n",
    "* BELOW I HAVE TESTED IF TRANSFORMS CALCULATE THE RANDOM PROBABILITY SEPERATELY FOR EVERY IMAGE IN A\n",
    "* ANSWER -> IT DOES NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T.RandomResizedCrop((image_size, image_size))\n",
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "params= t.get_params(img, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)) # returns top: int, left: int, height: int, width\n",
    "print(params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "t_color = T.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "params = t_color.get_params((0.2, 1.8), (0.2, 1.8), (0.2, 1.8), (-0.2, 0.2))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "class RandomHorizontalFlip(torch.nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        prob = torch.rand(1)\n",
    "        print(prob)\n",
    "        if prob < self.p:\n",
    "            return F.hflip(img)\n",
    "        return img\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "a = RandomHorizontalFlip()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "print(img.shape)\n",
    "_, height, width = F.get_dimensions(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "def random_hflip(img, p):\n",
    "    flip_bool = torch.rand(1) < p\n",
    "    return (F.hflip(img),flip_bool) if flip_bool else (img, flip_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "def transform_image(image, image_size):\n",
    "    # Apply color jitter with probability 0.3\n",
    "    if random.random() < 0.3:\n",
    "        image = TF.adjust_brightness(image, brightness_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_contrast(image, contrast_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_saturation(image, saturation_factor=random.uniform(0.2, 1.8))\n",
    "        image = TF.adjust_hue(image, hue_factor=random.uniform(-0.2, 0.2))\n",
    "\n",
    "    # Apply grayscale with probability 0.2\n",
    "    if random.random() < 0.2:\n",
    "        image = TF.to_grayscale(image)\n",
    "\n",
    "    # Apply horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        flipped_bool = True\n",
    "        image = TF.hflip(image)\n",
    "\n",
    "    # Apply gaussian blur with probability 0.2\n",
    "    if random.random() < 0.2:\n",
    "        image = TF.gaussian_blur(image, kernel_size=3, sigma=(1.0, 2.0))\n",
    "\n",
    "    # Apply random resized crop\n",
    "    crop_coordinates = top, left, height, width = T.RandomResizedCrop.get_params(\n",
    "        image, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.))\n",
    "    # if size is int (not list) smaller edge will be scaled to match this.\n",
    "    image = TF.resized_crop(image, top, left, height, width, size=(image_size, image_size))\n",
    "\n",
    "    # Normalize the image\n",
    "    image = TF.normalize(\n",
    "        torch.tensor(image, dtype=torch.float32),\n",
    "        mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "        std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "\n",
    "    return image, flipped_bool, crop_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()         \n",
    "\n",
    "    def forward(self, x):\n",
    "        r = random.random()\n",
    "        print(\"Hi ma\")        \n",
    "        print(r)\n",
    "        if r > 0:\n",
    "            x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "n = Net()\n",
    "# r = n(torch.tensor(-1))\n",
    "# print(r)\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "r = n.forward(img) #not planned to call directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "conv = torch.nn.Conv2d(3, 4, 2)\n",
    "\n",
    "at = torch.ones((3,4,3,60,60))\n",
    "print(at.size())\n",
    "bt = at.reshape(-1,*at.size()[-3:])\n",
    "print(bt.size())\n",
    "conv(bt).shape\n",
    "instance = F.interpolate(bt, size=(96, 96), mode=\"bicubic\") # resize instance to 96x96\n",
    "instance.shape\n",
    "torch.stack([bt,bt]).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming O and T are your input matrices of size KxM and KxN respectively.\n",
    "# Horizontal stack of column vectors of lenght K (K is number of instances)\n",
    "O_matrix = torch.tensor([[3,5,6],[4,12,8]], dtype=torch.float)# (M is number of features) \n",
    "T_matrix = torch.tensor([[1,2,3],[4,2,5]], dtype=torch.float)# (N is number of features)\n",
    "\n",
    "# Step 1: Compute DotProductMatrix\n",
    "DotProductMatrix = torch.mm(O_matrix.t(), T_matrix)\n",
    "\n",
    "# Step 2: Compute Norm matrices\n",
    "NormMatrixO = torch.norm(O_matrix, dim=0, keepdim=True)\n",
    "NormMatrixT = torch.norm(T_matrix, dim=0, keepdim=True)\n",
    "\n",
    "# Step 3: Compute C\n",
    "C = 1 - (DotProductMatrix / (NormMatrixO.t() * NormMatrixT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming O and T are your input matrices of size KxM and KxN respectively.\n",
    "# Horizontal stack of column vectors of lenght K (K is number of instances)\n",
    "O = torch.tensor([[3,5,6],[4,12,8]], dtype=torch.float)# (M is number of features) \n",
    "T = torch.tensor([[1,2,3],[4,2,5]], dtype=torch.float)# (N is number of features)\n",
    "\n",
    "# Step 1: Compute DotProductMatrix\n",
    "DotProductMatrix = torch.mm(O.t(), T)\n",
    "O.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute Norm matrices, output will be a horizontal vector.\n",
    "\n",
    "NormMatrixO = torch.norm(O, dim=0, keepdim=True) # find norm of columns\n",
    "NormMatrixT = torch.norm(T, dim=0, keepdim=True)\n",
    "NormMatrixO.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormMatrixT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO check if this (NormMatrixO.t() * NormMatrixT) matches the target indices.\n",
    "# Step 3: Compute C\n",
    "C = 1 - (DotProductMatrix / (NormMatrixO.t() * NormMatrixT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a and b for sinkhorn iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define two matrices\n",
    "A = torch.ones(3,4,6,6)\n",
    "B = torch.ones(3,4,6,6)\n",
    "\n",
    "# Perform elementwise multiplication\n",
    "C = torch.mul(A, B)\n",
    "print(C.shape)\n",
    "\n",
    "# Sum the results\n",
    "result = torch.sum(C, dim=(-2,-1))\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((torch.tensor([2,3,4,5]).unsqueeze(0), torch.tensor([2,3,4,5]).unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = torch.ones(0,4)\n",
    "bt =torch.ones(1,4)\n",
    "torch.cat((at,bt)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.ones(5,4))\n",
    "torch.max(torch.cat(torch.ones(5,4)[:, 0], torch.arange(1,6)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_boxes = []\n",
    "box= torch.tensor([1,2,3,4])\n",
    "filtered_boxes.append(box)\n",
    "filtered_boxes.append(box)\n",
    "filtered_boxes.append(box)\n",
    "at = torch.stack(filtered_boxes)\n",
    "at.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test concatentation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concatenated_instances(img, overlapping_boxes, instance_dim=0):\n",
    "    # Extract coordinates (b,k,4).T -> 4,b,k\n",
    "    x1, x2, y1, y2 = overlapping_boxes.T\n",
    "    print(x1.shape)\n",
    "    # Handle both cases (with or without batch dimension)\n",
    "    if instance_dim==1: # has batch dimension\n",
    "        crops = img[..., y1:y2, x1:x2] # (b, c, h, w)\n",
    "    else:\n",
    "        crops = img[:, None, y1:y2, x1:x2] # (c, 1, h, w)\n",
    "    \n",
    "    # Combine dimensions (b*n, c, h, w) or (n, c, h, w)\n",
    "    crops = crops.view(-1, *crops.shape[-3:])\n",
    "    \n",
    "    # Resize to 96x96\n",
    "    instances = F.interpolate(crops, size=(96, 96), mode=\"bicubic\")\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    if batched:\n",
    "        instances = instances.view(len(overlapping_boxes), -1, *instances.shape[-3:])\n",
    "    \n",
    "    # Stack the instances\n",
    "    return torch.stack(instances, dim=instance_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "overlapping_boxes = torch.tensor([[5,10,15,10],[6,10,15,10],[15,10,15,10],[55,10,15,10]])\n",
    "overlapping_boxes = overlapping_boxes.unsqueeze(dim=0).broadcast_to((3,*overlapping_boxes.shape))\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/data_ops/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(overlapping_boxes.shape)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates (b,k,4) permuted -> 4,k,b\n",
    "permuted_overlapping_boxes = torch.permute(overlapping_boxes, (2, 0, 1))\n",
    "instance_dim = img.ndim == 4\n",
    "print(permuted_overlapping_boxes.shape)\n",
    "x1, x2, y1, y2 = permuted_overlapping_boxes\n",
    "print(x1.shape)\n",
    "# Handle both cases (with or without batch dimension)\n",
    "if instance_dim==1: # has batch dimension\n",
    "    crops = img[:,:, y1:y2, x1:x2] # (b, c, h, w)\n",
    "else:\n",
    "    crops = img[:, None, y1:y2, x1:x2] # (c, 1, h, w)\n",
    "\n",
    "# Combine dimensions (b*n, c, h, w) or (n, c, h, w)\n",
    "crops = crops.view(-1, *crops.shape[-3:])\n",
    "\n",
    "# Resize to 96x96\n",
    "instances = F.interpolate(crops, size=(96, 96), mode=\"bicubic\")\n",
    "\n",
    "# Reshape back to original dimensions\n",
    "if batched:\n",
    "    instances = instances.view(len(overlapping_boxes), -1, *instances.shape[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "img = torch.randn(3, 5, 5)\n",
    "overlapping_boxes = torch.tensor([[5,10,15,10],[6,10,15,10],[15,10,15,10],[55,10,15,10]]).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "print(overlapping_boxes.shape)\n",
    "overlapping_boxes = overlapping_boxes.broadcast_to(3,5,5,4,4)\n",
    "x1,y1,x2,y2 = overlapping_boxes.broadcast_to(3,5,5,4,4)[:,:,:,:, 0],overlapping_boxes.broadcast_to(3,5,5,4,4)[:,:,:,:, 1],overlapping_boxes.broadcast_to(3,5,5,4,4)[:,:,:,:, 2],overlapping_boxes.broadcast_to(3,5,5,4,4)[:,:,:,:, 3]\n",
    "img[:,y1:y2, x1:x2]\n",
    "print(overlapping_boxes.shape)\n",
    "print(img.shape)\n",
    "# overlapping_boxes = overlapping_boxes.unsqueeze(dim=0).broadcast_to((3,*overlapping_boxes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.ops\n",
    "\n",
    "def get_concatenated_instances(img, overlapping_boxes):\n",
    "    # img shape should be (b, c, h, w) - batch, channels, height, width\n",
    "    # overlapping_boxes shape should be (b, n, 4) - batch, number of boxes, coordinates (x1, y1, x2, y2)\n",
    "\n",
    "    # Number of boxes per image\n",
    "    num_boxes = overlapping_boxes.size(1)\n",
    "    \n",
    "    # Create batch indices to be concatenated with boxes -> (batch_size*K), each box will have an index (showing where it belongs)\n",
    "    batch_indices = torch.arange(img.size(0), dtype=torch.float32).view(-1, 1).repeat(1, num_boxes).view(-1, 1)\n",
    "    print(batch_indices.shape)\n",
    "    print(batch_indices)\n",
    "    \n",
    "    # Reshape boxes for roi_align\n",
    "    boxes = overlapping_boxes.view(-1, 4).float() # Collect total number of boxes in the first dim (batch_size*K)\n",
    "    \n",
    "    print(boxes.shape)\n",
    "    # Concatenate batch indices with boxes, index shows which image in a batch each box belongs\n",
    "    boxes_with_indices = torch.cat([batch_indices, boxes], dim=1)\n",
    "    \n",
    "    # Crop and resize using roi_align\n",
    "    output_size = (96, 96)\n",
    "    instances = torchvision.ops.roi_align(img, boxes_with_indices, output_size)\n",
    "    \n",
    "    # Now instances tensor has shape (b * n, c, 96, 96), TODO you can reshape it if needed\n",
    "    return instances\n",
    "\n",
    "# Example usage\n",
    "b, c, h, w = 3, 3, 256, 256\n",
    "img_batch = torch.randn(b, c, h, w)\n",
    "overlapping_boxes_batch = torch.tensor([[[50, 50, 100, 100], [30, 30, 80, 80]], [[10, 10, 60, 60], [20, 20, 70, 70]], [[40, 40, 110, 110], [50, 50, 120, 120]]])\n",
    "\n",
    "# instances_batch = get_concatenated_instances(img_batch, overlapping_boxes_batch)\n",
    "# print(instances_batch.shape)  # Should be (b * n, c, 96, 96)\n",
    "img = torch.randn(1, c, h, w)\n",
    "overlapping_boxes = torch.tensor([[[50, 50, 100, 100], [30, 30, 80, 80]]])\n",
    "instances = get_concatenated_instances(img, overlapping_boxes)\n",
    "print(instances.shape)  # Should be (b * n, c, 96, 96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([*overlapping_boxes.shape[0:2], *instances.shape[-3:]])\n",
    "instances = instances.reshape(*overlapping_boxes.shape[0:2], *instances.shape[-3:])\n",
    "torch.concat(instances, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "T_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "dot_product_matrix = torch.matmul(O_matrix, T_matrix.transpose(1, 2))  # (batch_size, number of features, number of features)\n",
    "norm_vector_O = torch.norm(O_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "norm_vector_T = torch.norm(T_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "print(norm_vector_O.shape)\n",
    "print(norm_vector_T.shape)\n",
    "torch.matmul(norm_vector_O, norm_vector_T.transpose(1, 2)).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SINKHORN DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute dot_product_matrix\n",
    "online_pred_avg = torch.randn(3, 1, 5)  # (batch_size, instance numbers K, number of features)\n",
    "O_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "target_proj_avg = torch.randn(3, 1, 5)  # (batch_size, instance numbers K, number of features)\n",
    "T_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "dot_product_matrix = torch.matmul(O_matrix, T_matrix.transpose(1, 2))  # (batch_size, instance numbers K, instance numbers K)\n",
    "# Step 2: Compute Norm matrices\n",
    "norm_vector_O = torch.norm(O_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "norm_vector_T = torch.norm(T_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "# Step 3: Compute C\n",
    "ot_cosine_similarity_matrix = (dot_product_matrix / torch.matmul(norm_vector_O, norm_vector_T.transpose(1, 2)))\n",
    "cost_matrix = 1 - ot_cosine_similarity_matrix\n",
    "a_vector = torch.nn.functional.relu(torch.matmul(T_matrix, online_pred_avg.transpose(1, 2))) # (batch_size, instance numbers K, 1)\n",
    "print(a_vector.values.shape)\n",
    "print(torch.matmul(O_matrix, target_proj_avg.transpose(1, 2)).shape)\n",
    "b_vector = torch.nn.functional.relu(torch.matmul(O_matrix, target_proj_avg.transpose(1, 2))) # (batch_size, instance numbers K, 1)\n",
    "print(b_vector.shape)\n",
    "print(b_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = torch.randn(1, 4, 1)  # (batch_size, instance numbers K, number of features)\n",
    "at.squeeze(dim=-1).shape # demander a (batch_size, instance numbers K, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, instance numbers K, instance numbers K)\n",
    "optimal_plan_matrix = torch.randn(5, 4, 4)  # (batch_size, instance numbers K, number of features)\n",
    "ot_cosine_similarity_matrix = torch.randn(5, 4, 4)  # (batch_size, instance numbers K, number of features)\n",
    "torch.sum(-torch.mul(optimal_plan_matrix,ot_cosine_similarity_matrix), dim=(-2,-1)).mean() # Forces similar instance representations to be close to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
